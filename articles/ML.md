ML(Machine Learning)，即机器学习，这一部分的知识我之前一直是零零散散地在学，在这里稍微完整地整理一下几种基本的算法，主要注重于理论公式的推导和原理的理解。

目录如下：
- [线性回归](#线性回归)
  - [几种梯度下降](#几种梯度下降)
  - [正则化](#正则化)
- [逻辑回归](#逻辑回归)
  - [理解交叉熵损失](#理解交叉熵损失)
  - [混淆矩阵](#混淆矩阵)
  - [ROC曲线和AUC值](#roc曲线和auc值)
- [KNN算法](#knn算法)
  - [交叉验证和网格搜索](#交叉验证和网格搜索)
- [决策树](#决策树)
  - [ID3算法](#id3算法)
  - [C4.5算法](#c45算法)
  - [CART算法](#cart算法)
- [集成学习——随机森林](#集成学习随机森林)
- [集成学习——AdaBoost](#集成学习adaboost)
- [未完待续...](#未完待续)

## 线性回归

线性回归是机器学习中最基础的一种算法，主要用来解决回归问题，即预测连续值的问题。线性回归的模型可以表示为：

$$
\hat{y} = w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b
$$

其中，$\vec{x_i}=(x_{i1}, x_{i2}, ..., x_{in})$ 是一个样本，$x_{ik}$表示第$i$个样本的第$k$个特征，$w_k$ 是对应的权重，$b$ 是偏置项。线性回归的目标是找到一个合适的权重向量 $w$ 和偏置项 $b$，使得预测值 $\hat{y}$ 与真实值 $y$ 之间的误差最小。常用的损失函数是均方误差（MSE）：

$$
J(\vec{w}, b) = \frac{1}{2m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))^2
$$

其中$\vec{w} = (w_1, w_2, ..., w_n)$，$n$是特征列的数量，$m$ 是样本数量。
为了求解这个损失函数的最小值，我们可以使用梯度下降法。梯度下降法的核心思想是沿着损失函数的负梯度方向更新权重和偏置项，直到损失函数收敛到一个最小值。具体来说，对于权重 $w_i$ 和偏置项 $b$，我们可以使用以下公式进行更新：

$$
w_k = w_k - \alpha \frac{\partial J(\vec{w}, b)}{\partial w_k},\quad b = b - \alpha \frac{\partial J(\vec{w}, b)}{\partial b}
$$

这里计算一下梯度：

$$
\frac{\partial J(\vec{w}, b)}{\partial w_k} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))x_{ik}
$$

$$
\frac{\partial J(\vec{w}, b)}{\partial b} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))
$$

其中，$\alpha$ 是学习率，用于控制每次更新的步长。通过不断迭代，我们可以找到使损失函数最小的权重和偏置项，从而得到线性回归模型的预测结果。

### 几种梯度下降

梯度下降法有多种变体，下面介绍几种常见的梯度下降法。

**批量梯度下降（Batch Gradient Descent）**：批量梯度下降法是最基本的梯度下降法。它每次迭代时，都会使用**所有的训练样本**计算梯度，然后更新参数。由于每次迭代都需要计算所有样本的梯度，因此批量梯度下降法的计算量较大，但收敛速度较快。其更新公式为：

$$
\vec{w} = \vec{w} - \alpha \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i) \vec{x_i}
$$

其中，$m$ 是样本数量，$\alpha$ 是学习率，$w$ 是权重，$x_i$ 是第 $i$ 个样本的特征向量，$y_i$ 是第 $i$ 个样本的标签，$\hat{y}_i$ 是预测值。

**随机梯度下降（Stochastic Gradient Descent）**：随机梯度下降法每次迭代时，只使用**一个样本**计算梯度，然后更新参数。由于每次迭代只使用一个样本，因此随机梯度下降法的计算量较小，但收敛速度较慢。其更新公式为：

$$
\vec{w} = \vec{w} - \alpha (y_i - \hat{y}_i) \vec{x_i}
$$

其中，$x_i$ 是第 $i$ 个样本的特征向量，$y_i$ 是第 $i$ 个样本的标签，$\hat{y}_i$ 是预测值。

**小批量梯度下降（Mini-Batch Gradient Descent）**：小批量梯度下降法是批量梯度下降法和随机梯度下降法的折中。它每次迭代时，使用**一小批样本**计算梯度，然后更新参数。小批量梯度下降法的计算量介于批量梯度下降法和随机梯度下降法之间，收敛速度也介于两者之间。其更新公式为：

$$
\vec{w} = \vec{w} - \alpha \frac{1}{b} \sum_{i=1}^{b} (y_i - \hat{y}_i) \vec{x_i}
$$

其中，$b$ 是小批量的大小，$x_i$ 是第 $i$ 个样本的特征向量，$y_i$ 是第 $i$ 个样本的标签，$\hat{y}_i$ 是预测值。

### 正则化

正则化是一种防止过拟合的方法，它通过在损失函数中添加一个正则项来限制模型的复杂度。常见的正则化方法有L1正则化和L2正则化。这里以线性回归为例进行介绍。

**L1正则化**：L1正则化在损失函数中添加了权重的绝对值之和作为正则项。其公式为：

$$
J(\vec{w}, b) = \frac{1}{2m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))^2 + \lambda \sum_{k=1}^{n} |w_k|
$$

梯度如下：

$$
\frac{\partial J(\vec{w}, b)}{\partial w_k} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))x_{ik} + \lambda \text{sign}(w_k)
$$

其中，$\lambda$ 是正则化参数，用于控制正则项的权重。

**L2正则化**：L2正则化在损失函数中添加了权重的平方和作为正则项。其公式为：

$$
J(\vec{w}, b) = \frac{1}{2m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))^2 + \lambda \sum_{k=1}^{n} w_k^2
$$

梯度如下：

$$
\frac{\partial J(\vec{w}, b)}{\partial w_k} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - (w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b))x_{ik} + 2\lambda w_k
$$

其中，$\lambda$ 也是正则化参数，用于控制正则项的权重。


## 逻辑回归

逻辑回归是一种用于分类问题的机器学习算法，它通过将线性回归的结果映射到0和1之间，从而实现分类。逻辑回归的模型可以表示为：

$$
\hat{y} = \sigma(w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b)
$$

其中，$\sigma$ 是sigmoid函数，其定义为：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

其中，$z$ 是线性回归的结果。逻辑回归的目标是找到一个合适的权重向量 $w$ 和偏置项 $b$，使得预测值 $\hat{y}$ 与真实值 $y$ 之间的误差最小。常用的损失函数是交叉熵损失函数：

$$
J(\vec{w}, b) = -\frac{1}{m}\sum_{i=1}^{m}[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$

其中，$\vec{w} = (w_1, w_2, ..., w_n)$，$n$是特征列的数量，$m$ 是样本数量。

为了求解这个损失函数的最小值，我们也可以使用梯度下降法。具体来说，对于权重 $w_i$ 和偏置项 $b$，我们可以使用以下公式进行更新：

$$
w_k = w_k - \alpha \frac{\partial J(\vec{w}, b)}{\partial w_k},\quad b = b - \alpha \frac{\partial J(\vec{w}, b)}{\partial b}
$$

其中，$\alpha$ 是学习率，用于控制每次更新的步长。

### 理解交叉熵损失

假设我们有一个二分类问题，其中每一个样本 $\vec{x_i}$ 要么属于类别 $1$ ，要么属于类别 $0$。那么，对于给定的样本，模型预测其属于类别 $1$ 的概率为：

$$
p(y_i=1|\vec{x_i}) = \sigma(w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b) = \hat{y_i}
$$

其中，$\sigma$ 是sigmoid函数，$w$ 是权重向量，$b$ 是偏置项。

对于给定的样本 $\vec{x_i}$，模型预测其属于类别 $0$ 的概率为：

$$
p(y_i=0|\vec{x_i}) = 1 - \sigma(w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b) = 1 - \hat{y_i}
$$

那么，对于给定的样本 $\vec{x_i}$，对上面两个公式进行统一：

$$
p(y_i|\vec{x_i}) = p(y_i=1|\vec{x_i})^{y_i} \cdot p(y_i=0|\vec{x_i})^{1-y_i}
$$

其中，$y_i$ 是样本 $\vec{x_i}$ 的真实类别。这个公式表示：模型将1类样本归为1类并且将0类样本归为0类的概率，即**模型能正确预测该样本的概率（准确率）**。

对于整个数据集，模型能正确预测每一个样本的概率为（这个概率取决于模型本身，因此它是模型参数的函数）：

$$
L(\vec{w}, b) = \prod_{i=1}^{m} p(y_i|\vec{x_i})
$$

其中，$m$ 是样本数量。为了方便计算，取对数：

$$
\log L(\vec{w}, b) = \sum_{i=1}^{m} \log p(y_i|\vec{x_i})
$$

将 $p(y_i|\vec{x_i})$ 的表达式代入上式，得到：

$$
\log L(\vec{w}, b) = \sum_{i=1}^{m} [y_i \log(\hat{y_i}) + (1 - y_i) \log(1 - \hat{y_i})]
$$

交叉熵损失函数：
$$
J(\vec{w}, b) = -\frac{1}{m}\log L(\vec{w}, b) = \frac{1}{m}\sum_{i=1}^{m}[-y_i \log(\hat{y}_i) - (1 - y_i) \log(1 - \hat{y}_i)]
$$

我们来理解一下这个式子。对于样本$\vec{x_i}$，假设它事实上属于1类，即$y_i=1$，这时如果模型预测的$\hat{y_i}$越接近1，那么$-\log(\hat{y}_i)$就会越小，这一项的损失就小；反之如果模型预测的$\hat{y_i}$越接近0，那么$-\log(\hat{y}_i)$就会越大，这一项的损失就会显著增大。

### 混淆矩阵

混淆矩阵是用于评估分类模型性能的一种工具，它可以帮助我们了解模型在各个类别上的表现。混淆矩阵是一个 $2 \times 2$ 的表格，其中包含了四个指标：真正例（True Positive，TP）、假正例（False Positive，FP）、真负例（True Negative，TN）和假负例（False Negative，FN）。
|  | 真实值=1 | 真实值=0 |
| --- | --- | --- |
| 预测值=1 | TP | FP |
| 预测值=0 | FN | TN |

下面是几个常用指标：

**准确率（Accuracy）**：被模型正确预测的样本占总样本的比例。准确率越高，说明模型的整体性能越好。

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**精确率（Precision）**：被模型预测为类别1的样本中，实际属于类别1的样本所占的比例。精确率越高，说明模型在预测类别1时越准确。

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**召回率（Recall）**：实际属于类别1的样本中被模型正确预测为类别1的样本所占的比例。召回率越高，说明模型在预测类别1时越全面。

$$
\text{Recall} = \frac{TP}{TP + FN}
$$
**F1分数（F1 Score）**：精确率和召回率的调和平均数，用于综合评估模型在各个类别上的表现。F1分数越高，说明模型在预测类别1时越准确且全面。

$$
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

### ROC曲线和AUC值
ROC曲线（Receiver Operating Characteristic Curve）是用于评估二分类模型性能的一种工具，它可以帮助我们了解模型在不同阈值下的表现。ROC曲线是一条曲线，其横轴为假正例率（False Positive Rate，FPR），纵轴为真正例率（True Positive Rate，TPR）。

**假正例率（误报率，False Positive Rate，FPR）**：实际属于类别0的样本中，被模型预测为类别1的样本所占的比例。

$$
\text{FPR} = \frac{FP}{FP + TN}
$$
**真正例率（召回率，True Positive Rate，TPR）**：实际属于类别1的样本中，被模型正确预测为类别1的样本所占的比例。

$$
\text{TPR} = \frac{TP}{TP + FN}
$$

ROC曲线的横轴为FPR，纵轴为TPR，当模型预测的阈值从0到1变化时，FPR和TPR也会随之变化，从而形成一条曲线。ROC曲线越靠近左上角，说明模型的性能越好。
AUC值（Area Under Curve）是ROC曲线下面积，用于综合评估模型在各个阈值下的表现。AUC值越接近1，说明模型的性能越好。

> 我们通过ROC曲线坐标系的四个角落来进行理解。
> - 坐标(0,0)表示模型预测所有样本都属于类别0，此时误报率和召回率都为0。
> - 坐标(1,1)表示模型预测所有样本都属于类别1，此时误报率和召回率都为1。
> - 坐标(0,1)表示模型预测所有样本都正确，此时误报率为0，召回率为1。
> - 坐标(1,0)表示模型预测所有样本都错误，此时误报率为1，召回率为0。

## KNN算法

KNN（K-Nearest Neighbors，K近邻）算法是一种基于实例的学习算法，它通过比较待分类样本与已知样本之间的距离，来决定待分类样本的类别。KNN算法的基本思想是：如果一个待分类样本的K个最近邻样本中，大多数属于某个类别，那么这个待分类样本也属于这个类别。

KNN算法的主要步骤如下：

1. 计算待分类样本与已知样本之间的距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。
2. 选择一个合适的K值，即最近邻样本的数量。K值的选择会影响模型的性能，如果K值太小，模型可能会过拟合；如果K值太大，模型可能会欠拟合。
3. 根据K值，找出待分类样本的K个最近邻样本。
4. 根据K个最近邻样本的类别，决定待分类样本的类别。常用的决策方法有投票法、加权投票法等。
5. 重复步骤1-4，直到所有待分类样本都被分类。
   
### 交叉验证和网格搜索

交叉验证是一种常用的模型评估方法，将数据集旋转使用，减少因单次数据划分带来的评估偏差，更真实地估计模型在未知数据上的表现（泛化能力）。
> - K折交叉验证：将数据集划分为K个等份，每次选择其中一份作为验证集，其余K-1份作为训练集，进行K次训练和验证，最终取K次验证结果的平均值作为模型性能的评估指标。
> - 分层K折交叉验证：在K折交叉验证的基础上，保证每一折中各类样本的比例与原始数据集的比例相同，以避免因数据划分带来的偏差。
> - 留一法交叉验证：K折交叉验证的极端情况，取K等于样本数量。
> - 留P法交叉验证：每次留P个样本作为验证集，其余样本作为训练集。

网格搜索是一种常用的超参数优化方法，它通过遍历一组预定义的超参数组合，来找到最优的超参数组合。具体来说，对于每个超参数，我们定义一个取值范围，然后在每个取值范围内选择一个值，从而得到一个超参数组合。最后，我们遍历所有超参数组合，找到最优的超参数组合。

## 决策树

决策树是一种常用的分类和回归算法，它通过构建一棵树来对数据进行分类或回归。决策树的基本思想是：从根节点开始，根据某个特征对数据进行划分，然后递归地对每个子集进行划分，直到满足停止条件（如子集中的样本都属于同一个类别，或者子集中的样本数量小于某个阈值）。

决策树的主要步骤如下：

1. 选择一个特征，将数据集划分为多个子集。常用的特征选择方法有信息增益、信息增益比、基尼指数等。
2. 对每个子集递归地执行步骤1，直到满足停止条件。

### ID3算法

ID3（Iterative Dichotomiser 3）算法是一种基于**信息增益**的决策树算法。它通过计算每个特征的信息增益，来选择最优的特征作为决策树的节点。具体来说，ID3算法的主要步骤如下：

1. 计算数据集的**信息熵**。信息熵是衡量数据集的不确定性的指标，信息熵越大，说明数据集的不确定性越大。

$$
H(D) = -\sum_{i=1}^{n} p_i \log_2 p_i
$$

其中，$n$ 是类别的数量，$p_i$ 是第 $i$ 类样本在数据集中的比例。

> **信息熵**
> 先考虑单个事件的信息量，事件发生的概率为 $p$，则事件的信息量 $I(p)=-\log_2(p)$，单位为比特。例如，一个硬币正面朝上的概率是0.5，那么正面朝上的信息量是 $I(0.5)=-\log_2(0.5)=1$ 比特，说明需要1比特的信息才能确定硬币朝上的结果。
> 信息熵就是将所有可能事件的信息量，按其发生概率进行加权平均。

2. 计算每个特征的信息增益。信息增益是衡量特征对数据集不确定性的减少量的指标，信息增益越大，说明特征对数据集的不确定性减少量越大。

$$
Gain(D, A) = H(D) - \sum_{v \in A} \frac{|D_v|}{|D|} H(D_v)
$$

其中，$A$ 是特征，$v$ 是特征 $A$ 的取值，$D_v$ 是特征 $A$ 取值为 $v$ 的样本子集，$D$ 是数据集。

3. 选择信息增益最大的特征作为决策树的节点，并将数据集划分为多个子集。

4. 对每个子集递归地执行步骤1-3，直到满足停止条件（如子集中的样本都属于同一个类别，或者子集中的样本数量小于某个阈值）。

5. 将决策树转换为规则集，以便于理解和解释。

### C4.5算法

C4.5算法是一种基于**信息增益率**的决策树算法。它通过计算每个特征的信息增益率，来选择最优的特征作为决策树的节点。具体来说，C4.5算法的主要步骤如下：

1. 计算数据集的**信息熵**和信息增益，与ID3算法一样。

2. 计算每个特征的信息增益率。信息增益率是衡量特征对数据集不确定性的减少量的指标，信息增益率越大，说明特征对数据集的不确定性减少量越大。

$$
Gain\_{ratio}(D, A) = \frac{Gain(D, A)}{H_A(D)}
$$

其中，$H_A(D)$ 是特征 $A$ 的**固有值**，表示特征 $A$ 对数据集的不确定性增加量。特征取值越多，这个值越大。

$$
H_A(D) = -\sum_{v \in A} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}
$$

3. 选择信息增益率最大的特征作为决策树的节点，并将数据集划分为多个子集。

4. 对每个子集递归地执行步骤1-3，直到满足停止条件（如子集中的样本都属于同一个类别，或者子集中的样本数量小于某个阈值）。

5. 将决策树转换为规则集，以便于理解和解释。

> ID3算法更加偏好于选择取值较多的特征，而C4.5算法通过引入信息增益率，对特征取值进行了惩罚，从而避免了ID3算法的偏好问题。

### CART算法

CART（Classification and Regression Trees）算法是一种基于**基尼指数**的决策树算法。它通过计算每个特征对数据集的**基尼指数**，来选择最优的特征作为决策树的节点。具体来说，CART算法的主要步骤如下：

1. 计算数据集的**基尼指数**。基尼指数是衡量数据集的不确定性的指标，基尼指数越小，说明数据集的不确定性越小。

$$
Gini(D) = 1 - \sum_{i=1}^{n} p_i^2
$$

其中，$n$ 是类别的数量，$p_i$ 是第 $i$ 类样本在数据集中的比例。

2. 计算每个特征对数据集的基尼指数。基尼指数是衡量特征对数据集不确定性的减少量的指标，基尼指数越小，说明特征对数据集的不确定性减少量越大。

$$
Gini(D, A) = \sum_{v \in A} \frac{|D_v|}{|D|} Gini(D_v)
$$

其中，$A$ 是特征，$v$ 是特征 $A$ 的取值，$D_v$ 是特征 $A$ 取值为 $v$ 的样本子集，$D$ 是数据集。

3. 选择基尼指数最小的特征作为决策树的节点，并将数据集划分为多个子集。

4. 对每个子集递归地执行步骤1-3，直到满足停止条件（如子集中的样本都属于同一个类别，或者子集中的样本数量小于某个阈值）。

5. 将决策树转换为规则集，以便于理解和解释。

## 集成学习——随机森林

> 集成学习有三种方法：Boosting（序列集成）、Bagging（并行集成）和Stacking。

随机森林（Random Forest）是一种常用的集成学习方法，属于Bagging，它通过构建多个决策树，并将它们的预测结果进行投票或平均，来提高模型的性能。具体来说，随机森林的主要步骤如下：

1. 从原始数据集中随机选择一部分样本，构建一个决策树。每次构建决策树时，随机选择一部分特征，用于划分样本。
2. 重复步骤1，构建多个决策树。
3. 对于一个新的样本，将多个决策树的预测结果进行投票或平均，得到最终的预测结果。

随机森林的主要优点如下：
- 可以处理高维数据，并且不需要进行特征选择。
- 可以处理缺失值和异常值。
- 可以处理非线性关系。
- 可以处理不平衡数据集。

随机森林的主要缺点如下：
- 预测结果可能不稳定，因为决策树的数量和特征的选择都是随机的。
- 预测结果可能过于复杂，难以解释。

## 集成学习——AdaBoost

AdaBoost属于Boosting，核心思想：将多个“弱分类器”（仅比随机猜测略好，如单层Cart决策树）组合成一个强分类器。其自适应体现在：后续的弱分类器会更加关注前序分类器分错的样本。

以二分类为例，AdaBoost的主要步骤如下：

1. 初始化样本权重，每个样本的权重相等。

$$
D_1(i) = \frac{1}{N}, \quad i=1,2,\cdots,N
$$

其中，$N$ 是样本数量。

2. 对于每个弱分类器，计算样本的预测误差，并根据预测误差调整样本权重。预测误差越大的样本，权重越大（通过这种方式使得下一个弱分类器更加关注当前分错的样本）。

$$
\epsilon_t = P(y_t \neq f_t(x_i)) = \sum_{i=1}^{N} D_t(i) \cdot I(y_i \neq f_t(x_i))
$$

$$
\alpha_t = \frac{1}{2} \ln \frac{1 - \epsilon_t}{\epsilon_t}
$$

$$
D_{t+1}(i) = D_t(i) \cdot \frac{\exp(-\alpha_t y_t f_t(x_i))}{Z_t}, \quad i=1,2,\cdots,N
$$

其中，$t$ 是迭代次数，$y_t$ 是样本 $x_i$ 的真实标签（1或-1），$f_t(x_i)$ 是弱分类器 $f_t$ 对样本 $x_i$ 的预测结果，$\epsilon_t$ 是弱分类器预测的平均错误率，$D_t(i)$ 是第 $t$ 次迭代时样本 $i$ 的权重，$\alpha_t$是第$t$个弱分类器的权重，$Z_t$用于归一化。

3. 根据样本权重，训练下一个弱分类器。
4. 对每个弱分类器的预测结果进行加权平均，得到最终的预测结果。

$$
f(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t f_t(x)\right)
$$

其中，$T$ 是弱分类器的数量，$\alpha_t$ 是第 $t$ 个弱分类器的权重，$f_t(x)$ 是第 $t$ 个弱分类器对样本 $x$ 的预测结果。

## 未完待续...